<!DOCTYPE html>
<html>
    <head>
        <title>SVM for architects and other dummies :D</title>
        <link rel="stylesheet" href="main.css">
    </head>
    <body>
        <div class="nav_wrapper">

            <div class="nav_bar">
                <h1 class="title">S V M</h1>

                <nav>
                    <ul>
                        <li>
                            <a href="#">• WHAT IS SVM</a>
                        </li>
                        <li>
                            <a href="#">• LEARN HOW</a>
                        </li>
                        <li>
                            <a href="#">• ARCH APPROACHES</a>
                        </li>
                        <li>
                            <a href="#">• REFERENCES</a>
                        </li>
                    </ul>
                </nav>
            </div>

            <div class="main_wrapper">
                <div class="text_container">
                    <h2>WHAT IS SVM?</h2>
                    <h3>OVERVIEW</h3>
                    <p class="paragraph">     
                    From the name Support Vector Machines, SVM is a machine learning algorithm that can classify multidimensional data. This is done using a <span class="keyword" data-type="hyperplane">hyperplane</span>, which is essentially a decision boundary, that will divide and separate the data points into clear and well-defined groups.
                    
                    The term ‘<span class="keyword" data-type="support_vectors">Support Vectors</span>’ refers to the points from the data groups that have the shortest Euclidean distance to the hyperplane, defining a <span class="keyword" data-type="margin">margin</span>. This determines how well the model is performing. The larger the margin’s width, the better the model as the data groups will be well defined and the predictions will be more accurate.
                    </p>
                    <div class="animation">
                        <canvas class="webgl1"></canvas>
                        <script type="module" src="./figure1.js"></script>
                    </div>
                        <h3>OPERATION</h3>
                        <p class="paragraph">
                            In very simple terms, the process consists of a series of operations where the hyperplane is placed in different positions between data points until it finds the placement the separates the points into distinct groups and has the largest possible margin. In the following animation we can appreciate an extremely slowed down version of said process.
                        </p>
                    <div class="animation">
                        <canvas class="webgl2"></canvas>
                        <script type="module" src="./figure2.js"></script>
                        <button class="button" id="playButton">play</button>
                    </div>
                        <p class="paragraph">The SVM model can also operate with ‘non-splitable’ data by using a feature called the ‘kernel trick’. This hyper parameter allows us to make the data split by accessing a higher dimensional plane where the data can be separated by the splitting plane.
                        </p>
                    <div class="animation">
                        <canvas class="webgl3"></canvas>
                        <script type="module" src="./figure3.js"></script>
                        <div class="slider">
                            <label for="rotationSlider">Kernel Trick:</label>
                            <input type="range" id="rotationSlider" min="2" max="3" step="1" value="1">  
                        </div>
                        
                    </div>

                </div>

                <div class="text_container">
                    <h2>HOW TO USE SVM</h2>
                    <h3>GENERAL CONSIDERATIONS</h3>
                    <p class="paragraph">
                        First, as with any supervised learning algorithm, it is key to have a good dataset with no missing values. It’s also crucial that these values are either numerical or categorical or can be converted into those data types. To achieve this, libraries like Pandas and sci-kit learn offer multiple methods for data amputation and imputation.
                    </p>
                    <h3>HYPER PARAMETERS</h3>
                    <p class="paragraph">
                        To properly work with SVM (or any other algorithm in any case) it is necessary to understand what are the editable parameters that we can manipulate and how the inputted values affect our predictions.
                    </p>
                    <p class="paragraph">
                        <span class="keyword">C:</span> This might be the most crucial of parameters in SVM. It allows us to control the tolerance for misclassification of data points to achieve a greater hyperplane margin. A lower C value allows for a larger margin and more potential for misclassifications. A larger C value will do the opposite.
                        Grid Search and Randomized Search are common methods for tunning the c value.
                    </p>
                    <p class="paragraph">
                        <span class="keyword">Kernel:</span> As we saw before with the ‘Kernel trick’, this hyperparameter allows us to handle data that can be separated linearly by bringing it to a higher dimensional space where linear separation is possible. The are many types of kernel application besides linearly:
                    </p>
                        <p class="para2">
                            <span class="keyword">Polynomial:</span> Used when data simply cannot be separated by a linear hyperplane. To use this kernel it is also necessary to tune the ‘Degree’ and ‘Coef0’ hyperparameters.
                        </p>
                        <p class="para2">
                            <span class="keyword">Radial Basis Function (RBF):</span> It is the standard non-linear kernel. It creates an area around the data points forming groups. It is affected by the Gamma(y) hyperparameter, where a smaller ‘y’ value means a loose perimeter around the points, while a larger value creates a tighter and more defined curve around the data groups.
                        </p>
                        <p class="para2">
                            <span class="keyword">Sigmoid:</span> This kernel creates a series of non-linear curves that separate the data points in a more detailed way than the linear classification method. For this kernel it is necessary to tune the Gamma and Coef0 hyperparameters.
                        </p>
                    <p class="paragraph">
                        <span class="keyword">Gamma(y):</span> It affects the radius of influence that the data points have. A large gamma value means a smaller and more precise radius. This can be beneficial for more precise results, but it can also lead to overfitting. 
                    </p>
                    <p class="paragraph">
                        <span class="keyword">Degree:</span> It affects the complexity of the polynomial kernel. A larger number creates more complexity.
                    </p>
                    <p class="paragraph">
                        <span class="keyword">Coef0:</span> in polynomial and sigmoid kernels it affects the position of the decision boundary. Positive value makes the boundary closer to the positive side of chart. A negative value does the opposite.
                    </p> 
                    <p class="paragraph">
                        <span class="keyword">Probability:</span> This is a Boolean parameter that, when set to True, allows the algorithm to output a probabilistic estimate.
                    </p>   
                    <p class="paragraph">
                        <span class="keyword">Max Iter:</span> Allows us to set the maximum number of iterations for training our algorithm. This can have a huge impact on the training time and the model fitting.
                    </p>   
                </div>
            </div>
    </body>
</html>